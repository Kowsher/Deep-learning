{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "work.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUPiNenyN2LF"
      },
      "source": [
        "import pandas as pd\r\n",
        "data = pd.read_csv('/content/IMDB Dataset.csv')\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCxFYWPpjZvM"
      },
      "source": [
        "import matplotlib as mpl\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import os\r\n",
        "import re\r\n",
        "import shutil\r\n",
        "import string\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import losses\r\n",
        "\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import sklearn\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "\r\n",
        "from tensorflow.keras import preprocessing\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "import pydot\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNzqO2Krjbx0",
        "outputId": "e1692f08-a498-400f-f817-0e700c4162c6"
      },
      "source": [
        "X = data.iloc[:, 0]\r\n",
        "y = data.iloc[:, 1]\r\n",
        "\r\n",
        "from sklearn import preprocessing\r\n",
        "le = preprocessing.LabelEncoder()\r\n",
        "y = le.fit_transform(y)\r\n",
        "y"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY7-Lwcljjsw",
        "outputId": "812790f3-ecf8-42c7-fdb1-b9f44b5e9b68"
      },
      "source": [
        "from bs4 import BeautifulSoup \r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import pandas as pd\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "\r\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
        "\r\n",
        "def review_wordlist(review, remove_stopwords=True):\r\n",
        "    # 1. Removing html tags\r\n",
        "    review_text = BeautifulSoup(review).get_text()\r\n",
        "    # 2. Removing non-letter.\r\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\r\n",
        "    # 3. Converting to lower case and splitting\r\n",
        "    words = review_text.lower().split()\r\n",
        "    # 4. Optionally remove stopwords\r\n",
        "    if remove_stopwords:\r\n",
        "        stops = set(stopwords.words(\"english\"))     \r\n",
        "        words = [w for w in words if not w in stops]\r\n",
        "    \r\n",
        "    return(words)\r\n",
        "# This function splits a review into sentences\r\n",
        "def review_sentences(review, tokenizer, remove_stopwords=True):\r\n",
        "    # 1. Using nltk tokenizer\r\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\r\n",
        "    sentences = []\r\n",
        "    # 2. Loop for each sentence\r\n",
        "    for raw_sentence in raw_sentences:\r\n",
        "        if len(raw_sentence)>0:\r\n",
        "            t = review_wordlist(raw_sentence,remove_stopwords)\r\n",
        "            t = \" \".join(t)\r\n",
        "            sentences.append(t)\r\n",
        "\r\n",
        "    # This returns the list of lists\r\n",
        "    return sentences\r\n",
        "sentences = []\r\n",
        "print(\"Parsing sentences from training set\")\r\n",
        "for review in X:\r\n",
        "    sentences.append( \" \".join(review_sentences(review, tokenizer)))\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, y, test_size = 0.2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.7/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSjsO3UpjkQw"
      },
      "source": [
        "vocab_size = 5000 # make the top list of words (common words)\r\n",
        "embedding_dim = 64\r\n",
        "max_length = 100\r\n",
        "trunc_type = 'post'\r\n",
        "padding_type = 'post'\r\n",
        "oov_tok = '<OOV>' # OOV = Out of Vocabulary\r\n",
        "training_portion = .8"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcxLwmqtj7Ya"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\r\n",
        "tokenizer.fit_on_texts(X_train)\r\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szsAP8RBkHkZ"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(X_train)\r\n",
        "\r\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB0OwnRDkQUt"
      },
      "source": [
        "validation_sequences = tokenizer.texts_to_sequences(X_test)\r\n",
        "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf46gVyumJwu"
      },
      "source": [
        "from sklearn import preprocessing\r\n",
        "le = preprocessing.LabelEncoder()\r\n",
        "training_label_seq  = le.fit_transform(y_train)\r\n",
        "validation_label_seq  = le.transform(y_test)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kncuWjhqm08e",
        "outputId": "7f0e7052-03d2-4491-c2d6-63ab27162c43"
      },
      "source": [
        "training_label_seq"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30E4Y8kVnWZW",
        "outputId": "3a082eab-45e9-4191-ceeb-5e3657af83bd"
      },
      "source": [
        "max_features =50000\r\n",
        "embedding_dim =64\r\n",
        "sequence_length = 100\r\n",
        "\r\n",
        "model = tf.keras.Sequential()\r\n",
        "model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, input_length=sequence_length,\\\r\n",
        "                                    embeddings_regularizer = regularizers.l2(0.005))) \r\n",
        "model.add(tf.keras.layers.Dropout(0.04))\r\n",
        "\r\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,dropout=0.02, recurrent_dropout=0.2,return_sequences=True,\\\r\n",
        "                                                             kernel_regularizer=regularizers.l2(0.005),\\\r\n",
        "                                                             bias_regularizer=regularizers.l2(0.005))))\r\n",
        "model.add(tf.keras.layers.Dropout(0.04))\r\n",
        "model.add(tf.keras.layers.Flatten())\r\n",
        "\r\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu',\\\r\n",
        "                                kernel_regularizer=regularizers.l2(0.001),\\\r\n",
        "                                bias_regularizer=regularizers.l2(0.001),))\r\n",
        "model.add(tf.keras.layers.Dropout(0.04))\r\n",
        "\r\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu',\\\r\n",
        "                                kernel_regularizer=regularizers.l2(0.001),\\\r\n",
        "                                bias_regularizer=regularizers.l2(0.001),))\r\n",
        "model.add(tf.keras.layers.Dropout(0.04))\r\n",
        "\r\n",
        "\r\n",
        "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\r\n",
        "                               \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model.summary()\r\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=tf.keras.optimizers.Adam(lr=0.001, decay=1e-6),metrics=['accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)))\r\n",
        "#model.add(tf.keras.layers.Dense(15, activation='sigmoid'))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 100, 64)           3200064   \n",
            "_________________________________________________________________\n",
            "dropout_47 (Dropout)         (None, 100, 64)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 100, 128)          66048     \n",
            "_________________________________________________________________\n",
            "dropout_48 (Dropout)         (None, 100, 128)          0         \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 12800)             0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 512)               6554112   \n",
            "_________________________________________________________________\n",
            "dropout_49 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 8)                 4104      \n",
            "_________________________________________________________________\n",
            "dropout_50 (Dropout)         (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 9,824,337\n",
            "Trainable params: 9,824,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjpHsFG9x_6w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5RaFvN_qLhN",
        "outputId": "dfc3b499-8bee-4dd6-b7d0-ce996103adcd"
      },
      "source": [
        "num_epochs = 10\r\n",
        "history = model.fit(train_padded, training_label_seq,  batch_size=128, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)\r\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "140/140 - 106s - loss: 2.8403 - accuracy: 0.7687 - val_loss: 1.0556 - val_accuracy: 0.8243\n",
            "Epoch 2/10\n",
            "140/140 - 100s - loss: 0.9298 - accuracy: 0.8519 - val_loss: 0.8911 - val_accuracy: 0.8312\n",
            "Epoch 3/10\n",
            "140/140 - 106s - loss: 0.7782 - accuracy: 0.8623 - val_loss: 0.7797 - val_accuracy: 0.8294\n",
            "Epoch 4/10\n",
            "140/140 - 102s - loss: 0.6869 - accuracy: 0.8621 - val_loss: 0.6963 - val_accuracy: 0.8419\n",
            "Epoch 5/10\n",
            "140/140 - 98s - loss: 0.6062 - accuracy: 0.8705 - val_loss: 0.6629 - val_accuracy: 0.8272\n",
            "Epoch 6/10\n",
            "140/140 - 97s - loss: 0.5458 - accuracy: 0.8705 - val_loss: 0.5793 - val_accuracy: 0.8399\n",
            "Epoch 7/10\n",
            "140/140 - 96s - loss: 0.5015 - accuracy: 0.8762 - val_loss: 0.5440 - val_accuracy: 0.8417\n",
            "Epoch 8/10\n",
            "140/140 - 96s - loss: 0.4681 - accuracy: 0.8805 - val_loss: 0.5258 - val_accuracy: 0.8393\n",
            "Epoch 9/10\n",
            "140/140 - 94s - loss: 0.4373 - accuracy: 0.8826 - val_loss: 0.5057 - val_accuracy: 0.8357\n",
            "Epoch 10/10\n",
            "140/140 - 95s - loss: 0.4233 - accuracy: 0.8827 - val_loss: 0.5067 - val_accuracy: 0.8381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8Tc3zBzvaow"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}